<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom"><id>https://github.com/imjuya/juya-ai-daily</id><title>橘鸦AI早报</title><updated>2026-02-19T01:57:41.831652+00:00</updated><author><name>imjuya</name><email>imjuyaya@gmail.com</email></author><link href="https://github.com/imjuya/juya-ai-daily"/><link href="https://raw.githubusercontent.com/imjuya/juya-ai-daily/master/feed.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>记录人类完蛋全过程</subtitle><entry><id>https://github.com/imjuya/juya-ai-daily/issues/2</id><title>2026-02-19</title><updated>2026-02-19T01:57:42.180293+00:00</updated><content type="html"><![CDATA[<p><img src="http://testtttt.oss-cn-guangzhou.aliyuncs.com/imagehub/20260219/20260219090351005487a008_cover_73fa.png" alt="" /></p>
<h1>AI 早报 2026-02-19</h1>
<p><strong>视频版</strong>：<a href="https://www.youtube.com/watch?v=qaQ1uDPjCqk">YouTube</a> ｜ <a href="https://www.bilibili.com/video/BV1WfZ8BqEu6">哔哩哔哩</a></p>
<h2>概览</h2>
<h3>模型发布</h3>
<ul>
<li>Google DeepMind发布Lyria 3音乐生成模型 <code>#1</code></li>
<li>xAI 发布16-Agent协作模式 Grok 4.20 Heavy <code>#2</code></li>
<li>Prime Intellect开源106B参数MoE模型 <code>#3</code></li>
<li>印度AI企业Sarvam发布Sarvam-30B与105B <code>#4</code></li>
</ul>
<h3>开发生态</h3>
<ul>
<li>OpenAI 开源 Codex App Server 支持ChatGPT登录 <code>#5</code></li>
<li>Figma 推出 Claude Code to Figma 功能 <code>#6</code></li>
<li>OpenAI发布智能合约安全基准EVMbench <code>#7</code></li>
<li>Gemini CLI 推出 v0.29.0 版本每周更新 <code>#8</code></li>
</ul>
<h3>行业动态</h3>
<ul>
<li>谷歌、OpenAI 和微软宣布在印计划 <code>#9</code></li>
<li>Perplexity撤下搜索平台广告 <code>#10</code></li>
<li>World Labs完成融资并与Autodesk合作 <code>#11</code></li>
</ul>
<h3>技术与洞察</h3>
<ul>
<li>智谱AI发布GLM-5技术报告 <code>#12</code></li>
</ul>
<hr />
<h2><a href="https://deepmind.google/models/lyria/">Google DeepMind发布Lyria 3音乐生成模型</a> <code>#1</code></h2>
<blockquote>
<p><strong>Google DeepMind</strong> 发布音乐生成模型 <code>Lyria 3</code>，并已在 <code>Gemini</code> 桌面端开启测试。用户只需输入文字描述或上传图片、视频，即可生成 <strong>30 秒</strong> 带歌词的 <code>高保真</code> 音乐，且支持对风格和人声的精细控制。该功能面向 <strong>18 岁</strong> 以上用户开放，生成的所有音轨均嵌入 <code>SynthID</code> 水印，以确保可追溯性。</p>
</blockquote>
<p><strong>Google DeepMind</strong> 发布最先进音乐生成模型 <strong>Lyria 3</strong>，并已在 <strong>Gemini App</strong> 中推出 Beta 版。用户可以通过 <code>文本转音轨</code> 功能，描述特定的流派、情绪、记忆甚至内部笑话来生成音乐；也可以利用 <code>图像/视频转音轨</code> 功能，上传照片或视频，让 AI 根据视觉内容的氛围自动谱曲并填写歌词。<strong>Gemini App</strong> 生成的音轨时长固定为 <strong>30</strong> 秒，并附带由 <code>Nano Banana</code> 生成的自定义封面图。</p>
<p>官方指出，<strong>Lyria 3</strong> 的 <strong>三大</strong> 改进点包括：自动生成歌词无需用户提供、提供对风格及人声和节奏的更强控制、以及生成更真实且音乐结构更复杂的曲目。该服务目前向所有 <strong>18</strong> 岁及以上的 <strong>Gemini</strong> 用户开放，支持英语、德语、西班牙语、法语、印地语、日语、韩语和葡萄牙语。桌面端现已可用，移动端预计将在未来几天内上线，<strong>Google AI Plus</strong>、<strong>Pro</strong> 和 <strong>Ultra</strong> 订阅用户将享有更高的使用额度。在 <strong>Gemini</strong> 应用中生成的所有音轨均嵌入了 <code>SynthID</code>，<strong>Gemini</strong> 亦上线了 <code>音频验证工具</code>。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/73facf84-16ad-4d1b-8f5a-f3decce3eae0/f050b0b0-bd7f-4176-b7e3-b545c975855d/m001.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://deepmind.google/models/lyria/">https://deepmind.google/models/lyria/</a></li>
<li><a href="https://blog.google/innovation-and-ai/products/gemini-app/lyria-3/">https://blog.google/innovation-and-ai/products/gemini-app/lyria-3/</a></li>
</ul>
<hr />
<h2><a href="https://x.com/elonmusk/status/2024194618930401590">xAI 发布16-Agent协作模式 Grok 4.20 Heavy</a> <code>#2</code></h2>
<blockquote>
<p><strong>Elon Musk</strong> 宣布<strong>xAI</strong>上线 <code>Grok 4.20 Heavy</code> 并向 Heavy 订阅用户开放。该模型核心架构为由 <strong>16</strong> 个专门化 <code>Agent</code> 组成的协作团队。</p>
</blockquote>
<p><strong>Elon Musk</strong>宣布推出<code>Grok 4.20 Heavy</code>，称其为重大升级，现已向<strong>Heavy</strong>订阅者开放。该版本核心变化在于底层架构调整，即在<code>grok-4.20架构</code>下运行由<strong>16</strong>个专门化<code>Agent</code>组成的团队进行<code>实时协作</code>。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/73facf84-16ad-4d1b-8f5a-f3decce3eae0/e0145bc4-033f-4528-bfb5-5821485d31b6/m001.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://x.com/elonmusk/status/2024194618930401590">https://x.com/elonmusk/status/2024194618930401590</a></li>
<li><a href="https://x.com/tetsuoai/status/2024121909286494435">https://x.com/tetsuoai/status/2024121909286494435</a></li>
</ul>
<hr />
<h2><a href="https://huggingface.co/PrimeIntellect/INTELLECT-3.1">Prime Intellect开源106B参数MoE模型</a> <code>#3</code></h2>
<blockquote>
<p><strong>Prime Intellect</strong> 发布了基于 <code>GLM-4.5-Air-Base</code> 通过 <code>强化学习</code> 训练的 <code>INTELLECT-3.1</code> 模型，显著提升了在数学、编程及 <code>Agent</code> 任务上的表现。</p>
</blockquote>
<p><strong>Prime Intellect</strong> 发布开源推理模型 <code>INTELLECT-3.1</code>。该模型采用 <code>Mixture-of-Experts (MoE)</code> 架构，拥有 <strong>106B</strong> 总参数及 <strong>A12B</strong> 活跃参数，基于 <code>zai-org/GLM-4.5-Air-Base</code> 并结合 <code>prime-rl</code> 框架进行了强化学习训练，重点提升了数学、编程、软件工程和 <code>Agent</code> 任务的能力。</p>
<p>官方已将模型、训练框架及环境以 <code>MIT</code> 和 <code>Apache 2.0</code> 协议完全开源，并提供了技术报告。</p>
<p>相关链接：</p>
<ul>
<li><a href="https://huggingface.co/PrimeIntellect/INTELLECT-3.1">https://huggingface.co/PrimeIntellect/INTELLECT-3.1</a></li>
</ul>
<hr />
<h2><a href="https://techcrunch.com/2026/02/18/indian-ai-lab-sarvams-new-models-are-a-major-bet-on-the-viability-of-open-source-ai">印度AI企业Sarvam发布Sarvam-30B与105B</a> <code>#4</code></h2>
<blockquote>
<p>印度AI初创公司<strong>Sarvam</strong>发布了从零训练的<code>Sarvam-30B</code>和<code>Sarvam-105B</code>混合专家架构模型。根据官方数据，这两款模型针对印度本土语言进行了优化，在多项基准测试中表现出色。</p>
</blockquote>
<p>印度AI初创公司<strong>Sarvam</strong>近日发布了<strong>两款</strong>从零训练的<code>大型语言模型``Sarvam-30B</code>和<code>Sarvam-105B</code>，并同步推出语音及视觉模型。核心模型采用<code>混合专家架构</code>，获<strong>印度政府</strong>、<strong>Yotta</strong>及<strong>Nvidia</strong>支持。其中，<strong>30B</strong>模型预训练数据达<strong>16T</strong> <code>Token</code>，支持<strong>32k</strong> <code>上下文</code>；<strong>105B</strong>模型支持<strong>128k</strong> <code>上下文</code>，专攻复杂推理。官方数据显示，其性能优于或持平<code>Gemma</code>、<code>Qwen</code>等竞品。<strong>Sarvam</strong>计划开源<strong>这两款</strong>模型，并推出**“Sarvam for Work”<strong>企业工具及</strong>“Samvaad”**对话<code>Agent</code>平台。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/73facf84-16ad-4d1b-8f5a-f3decce3eae0/d06d1a70-4751-4a0e-a57a-33b53050c804/m001.png" alt="" /></p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/73facf84-16ad-4d1b-8f5a-f3decce3eae0/d06d1a70-4751-4a0e-a57a-33b53050c804/m002.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://techcrunch.com/2026/02/18/indian-ai-lab-sarvams-new-models-are-a-major-bet-on-the-viability-of-open-source-ai">https://techcrunch.com/2026/02/18/indian-ai-lab-sarvams-new-models-are-a-major-bet-on-the-viability-of-open-source-ai</a></li>
</ul>
<hr />
<h2><a href="https://developers.openai.com/codex/app-server/">OpenAI 开源 Codex App Server 支持ChatGPT登录</a> <code>#5</code></h2>
<blockquote>
<p><strong>OpenAI</strong> 开源了驱动 Codex 应用的底层框架 <code>Codex App Server</code>，允许开发者将 <code>Codex Agent</code> 深度集成到自有产品中。该工具支持通过 <strong>ChatGPT</strong> 账号 <code>OAuth</code> 直接接入第三方应用。</p>
</blockquote>
<p><strong>OpenAI</strong> 已开源其核心接口 <code>Codex App Server</code>，旨在让开发者在自有产品中深度集成 <code>Codex Agent</code>。该服务器是 Codex 富客户端（如 <code>VS Code</code> 扩展）的底层驱动，支持通过 <strong>ChatGPT</strong> 账号 <code>OAuth</code> 直接接入第三方应用，并提供身份验证、会话历史、审批流和流式 Agent 事件等完整功能。在协议层面，其采用类似 <code>MCP</code> 的 <code>JSON-RPC 2.0</code> 进行双向通信，支持 <code>stdio</code> 和实验性的 <code>websocket</code> 传输方式。客户端必须在连接后立即发送 <code>initialize</code> 请求并携带标识信息，企业级集成需在 <strong>OpenAI</strong> 平台注册。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/73facf84-16ad-4d1b-8f5a-f3decce3eae0/f1458b74-fc09-407d-9990-8504ee226fb2/m001.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://developers.openai.com/codex/app-server/">https://developers.openai.com/codex/app-server/</a></li>
<li><a href="https://github.com/openai/codex/tree/main/codex-rs/app-server">https://github.com/openai/codex/tree/main/codex-rs/app-server</a></li>
</ul>
<hr />
<h2><a href="https://www.figma.com/blog/introducing-claude-code-to-figma/">Figma 推出 Claude Code to Figma 功能</a> <code>#6</code></h2>
<blockquote>
<p><strong>Figma</strong> 近期推出 <code>Claude Code to Figma</code> 功能，利用 <code>Figma MCP Server</code>，能够将 <code>Claude Code</code> 生成的 <code>UI</code> 直接转化为 <strong>Figma</strong> 画布上的可编辑 <code>Frame</code>。</p>
</blockquote>
<p><strong>Figma</strong> 近期推出“<code>Claude Code</code> to <strong>Figma</strong>”功能，通过更新<strong>Figma</strong><code>MCP server</code>，打通代码构建与设计协作。开发者可将<code>Claude Code</code>生成的真实<code>UI</code>转化为<strong>Figma</strong>画布上可编辑的<code>Frame</code>，支持捕捉完整流程并保留上下文，从而实现从“代码收敛”到“画布发散”的高效转换。该功能允许团队直接对代码生成的界面进行协作，避免了截图反馈的摩擦，并支持从设计回到代码的“往返”工作流。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/73facf84-16ad-4d1b-8f5a-f3decce3eae0/1aadc184-cda8-4630-858b-7a5f6b3a2db2/m001.gif" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://www.figma.com/blog/introducing-claude-code-to-figma/">https://www.figma.com/blog/introducing-claude-code-to-figma/</a></li>
</ul>
<hr />
<h2><a href="https://openai.com/index/introducing-evmbench/">OpenAI发布智能合约安全基准EVMbench</a> <code>#7</code></h2>
<blockquote>
<p><strong>OpenAI</strong> 与 <strong>Paradigm</strong> 联合推出了<code>智能合约安全基准</code> <code>EVMbench</code>，旨在评估 <code>AI Agent</code> 在检测、修补及利用高危漏洞方面的<code>实战能力</code>。</p>
</blockquote>
<p><strong>OpenAI</strong> 联合 <strong>Paradigm</strong> 发布 <code>EVMbench</code> 基准，旨在评估 <code>AI Agent</code> 在 <code>智能合约</code> 安全领域（<code>检测</code>、<code>修补</code>、<code>利用</code>）的能力。该基准包含 <strong>120</strong> 个源自 <strong>40</strong> 次审计的漏洞场景，并配备基于 <code>Rust</code> 的测试工具和 <code>隔离沙盒</code> 环境。</p>
<p>官方评估显示，<code>GPT-5.3-Codex</code> 在 <code>Exploit</code> 模式得分 <strong>72.2%</strong>，显著优于 <code>GPT-5</code> 的 <strong>31.9%</strong>，但在 <code>检测</code> 和 <code>修补</code> 方面仍有提升空间。<strong>OpenAI</strong> 承认该基准存在环境与评分机制局限，不完全代表现实难度。</p>
<p>相关链接：</p>
<ul>
<li><a href="https://openai.com/index/introducing-evmbench/">https://openai.com/index/introducing-evmbench/</a></li>
</ul>
<hr />
<h2><a href="https://github.com/google-gemini/gemini-cli/discussions/19473">Gemini CLI 推出 v0.29.0 版本每周更新</a> <code>#8</code></h2>
<blockquote>
<p><code>Gemini CLI</code> 推出 <code>v0.29.0</code> 版本更新，新增 <code>Ask User Tool</code> 支持模型交互式提问以澄清指令，同时引入 <code>Conductor</code> 和 <code>Firebase Agent Skills</code> 扩展，并上线了实验性 <code>Plan Mode</code>。</p>
</blockquote>
<p><strong>Gemini CLI</strong> 近日推出 <strong>v0.29.0</strong> 版本更新，新增 <code>Ask User Tool</code>，支持模型交互式暂停并询问用户以获取澄清。官方扩展了生态系统，引入 <strong>Conductor</strong> 扩展用于自动化逻辑审查与合规报告，以及 <strong>Firebase Agent Skills</strong> 扩展以优化开发体验。性能方面，更新优化了对海量工具输出的处理，防止 <code>上下文窗口</code> 过载，从而支持更长时间的高质量推理；同时改进了 <code>Vim</code> 编辑体验（支持 <code>W</code>、<code>B</code>、<code>E</code> 动作）及快捷键可发现性（按 <code>?</code> 唤起）。此外，官方首推实验性 <code>Plan Mode</code>，该只读模式旨在帮助用户在不修改文件的前提下映射代码库并验证假设。</p>
<p>相关链接：</p>
<ul>
<li><a href="https://github.com/google-gemini/gemini-cli/discussions/19473">https://github.com/google-gemini/gemini-cli/discussions/19473</a></li>
</ul>
<hr />
<h2><a href="https://blog.google/innovation-and-ai/technology/ai/ai-impact-summit-2026-india/">谷歌、OpenAI 和微软宣布在印计划</a> <code>#9</code></h2>
<blockquote>
<p>在新德里 <code>AI Impact Summit</code> 上，<strong>谷歌</strong>、<strong>OpenAI</strong> 和 <strong>微软</strong> 密集公布了它们在印度的最新计划。<strong>谷歌</strong>启动了连接美印的<code>战略光纤倡议</code>；<strong>OpenAI</strong> 联合当地顶尖高校，将 <code>ChatGPT Edu</code> 引入高等教育体系；<strong>微软</strong> 则承诺在印度培训 <strong>2000 万人</strong>。</p>
</blockquote>
<p>本周在新德里<strong>AI Impact Summit</strong>期间，<strong>Google</strong>、<strong>OpenAI</strong>和<strong>Microsoft</strong>密集公布在印扩展计划。<strong>Google</strong>启动“<strong>America-India Connect</strong>”光缆基建倡议，连接美印及南半球；<strong>Google.org</strong>投入共<strong>6000万美元</strong>用于科学及政府创新挑战赛，并深化在教育、农业及能源领域的<code>AI模型</code>部署。<strong>OpenAI</strong>通过与印度顶尖高校合作，将<code>ChatGPT Edu</code>整合进高等教育体系，覆盖逾<strong>10万</strong>师生。<strong>Microsoft</strong>宣布到本十年末向“<strong>全球南方</strong>”投资<strong>500亿美元</strong>，其中计划在<strong>2030年</strong>前培训<strong>2000万</strong>印度人。</p>
<p>相关链接：</p>
<ul>
<li><a href="https://blog.google/innovation-and-ai/technology/ai/ai-impact-summit-2026-india/">https://blog.google/innovation-and-ai/technology/ai/ai-impact-summit-2026-india/</a></li>
<li><a href="https://techcrunch.com/2026/02/18/openai-pushes-into-higher-education-as-india-seeks-to-scale-ai-skills">https://techcrunch.com/2026/02/18/openai-pushes-into-higher-education-as-india-seeks-to-scale-ai-skills</a></li>
</ul>
<hr />
<h2><a href="https://www.ft.com/content/6eec07a5-34a8-4f78-a9ed-93ab4263d43c">Perplexity撤下搜索平台广告</a> <code>#10</code></h2>
<blockquote>
<p>据报道，<strong>Perplexity</strong>将撤下平台所有广告，以确保用户信任和搜索准确性。公司战略重心现已转向订阅服务和企业销售，并计划积极扩充销售团队。</p>
</blockquote>
<p>据报道，AI 搜索初创公司 <strong>Perplexity</strong> 将从平台撤下广告，以维护用户信任，坚持“准确性企业”定位。该公司高管表示不会在聊天机器人答案中投放广告，未来将专注于订阅服务和企业销售。公司目前正锁定大型企业及财务、医疗等高端用户，企业销售计划积极扩张。</p>
<p>相关链接：</p>
<ul>
<li><a href="https://www.ft.com/content/6eec07a5-34a8-4f78-a9ed-93ab4263d43c">https://www.ft.com/content/6eec07a5-34a8-4f78-a9ed-93ab4263d43c</a></li>
</ul>
<hr />
<h2><a href="https://www.worldlabs.ai/blog/funding-2026">World Labs完成融资并与Autodesk合作</a> <code>#11</code></h2>
<blockquote>
<p><strong>李飞飞</strong>创立的 <strong>World Labs</strong> 宣布完成 <strong>10亿美元</strong> 新一轮融资，<strong>AMD</strong> 和 <strong>英伟达</strong> 等巨头参投。与此同时，<strong>Autodesk</strong>向其投入 <strong>2亿美元</strong> 并达成战略合作，计划将 <strong>World Labs</strong> 的 <code>空间智能模型</code> 与自家的 <code>3D 工具</code> 相结合。</p>
</blockquote>
<p><strong>李飞飞</strong>创立的AI初创公司<strong>World Labs</strong>近日宣布完成<strong>10亿美元</strong>新一轮融资，本轮融资由<strong>AMD</strong>、<strong>Autodesk</strong>、<strong>NVIDIA</strong>等机构共同参与。<strong>World Labs</strong>致力于通过构建<code>World Models</code>来推进空间智能发展。</p>
<p>软件设计巨头<strong>Autodesk</strong>在本轮融资中投资了<strong>2亿美元</strong>，并与<strong>World Labs</strong>达成战略合作。双方初期将聚焦于媒体和娱乐领域，探索将<strong>World Labs</strong>的模型与<strong>Autodesk</strong>的<code>3D工具</code>及正在开发的新一代生成式AI模型<code>Neural CAD</code>相结合，旨在为设计、建筑及娱乐行业提供更高级的物理AI解决方案。据<strong>Autodesk</strong>首席科学家<strong>Daron Green</strong>称，合作尚处早期阶段，且双方协议明确不包含数据共享。</p>
<p>相关链接：</p>
<ul>
<li><a href="https://www.worldlabs.ai/blog/funding-2026">https://www.worldlabs.ai/blog/funding-2026</a></li>
</ul>
<hr />
<h2><a href="https://arxiv.org/abs/2602.15763">智谱AI发布GLM-5技术报告</a> <code>#12</code></h2>
<blockquote>
<p><strong>智谱AI</strong>在arXiv发布了<code>GLM-5</code>技术报告。该模型采用<code>DSA架构</code>，并已适配<strong>7种</strong>国产芯片。官方宣称，通过<code>异步Agent RL基础设施</code>及<code>Slime RL工具包</code>等创新， <code>GLM-5</code> 在开源模型中实现了<code>SOTA性能</code>。</p>
</blockquote>
<p><strong>智谱AI</strong>近日在<strong>arXiv</strong>上发布<code>GLM-5</code>技术报告，详述其架构与训练细节。该模型采用<code>DSA</code>架构，拥有<strong>750B</strong>总参数与<strong>40B</strong>激活参数，训练数据量达<strong>30T</strong>，并已完成对<strong>7</strong>种国产芯片的适配。</p>
<p>报告揭示了多项核心创新。<code>DSA</code>架构旨在降低训练推理成本并保持长上下文保真度。模型引入<code>Slime RL</code>工具包与<code>异步Agent RL</code>基础设施，通过解耦生成与训练过程提升后训练效率。技术实现细节上，<code>GLM-5</code>的<code>RL</code>算法使用了带有双向<code>Token级掩码</code>的<code>GRPO</code>。为提升效率，系统采用<code>FP8</code>减少Token间延迟，实施平均接受长度为<strong>2.76</strong>的<code>多Token预测</code>，并利用<code>数据并行感知路由</code>最大化<code>KV-cache</code>复用。此外，<code>Prefill-Decode分离技术</code>可避免处理干扰。其<code>异步Agent RL</code>系统基于样本阈值更新模型，会主动丢弃过旧或崩溃环境的样本，并通过<code>Token-in Token-out</code>机制对齐推理与训练，<code>混合式奖励系统</code>则衡量基础正确性、情商与任务质量。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/73facf84-16ad-4d1b-8f5a-f3decce3eae0/f93b154d-a72b-4e76-8770-06eec166737e/m001.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://arxiv.org/abs/2602.15763">https://arxiv.org/abs/2602.15763</a></li>
<li><a href="https://x.com/Zai_org/status/2023951884826849777">https://x.com/Zai_org/status/2023951884826849777</a></li>
</ul>
<hr />
<p><strong>提示</strong>：内容由AI辅助创作，可能存在<strong>幻觉</strong>和<strong>错误</strong>。</p>
<p>作者<code>橘鸦Juya</code>，视频版在同名<strong>哔哩哔哩</strong>。欢迎<strong>点赞、关注、分享</strong>。</p>
]]></content><link href="https://github.com/imjuya/juya-ai-daily/issues/2"/><published>2026-02-19T01:57:23+00:00</published></entry><entry><id>https://github.com/imjuya/juya-ai-daily/issues/1</id><title>2026-02-18</title><updated>2026-02-19T01:57:42.352010+00:00</updated><content type="html"><![CDATA[<p><img src="http://testtttt.oss-cn-guangzhou.aliyuncs.com/imagehub/20260218/202602180850442485579dde_cover_e48a.jpg" alt="" /></p>
<h1>AI 早报 2026-02-18</h1>
<p><strong>视频版</strong>：<a href="https://www.youtube.com/watch?v=8jAigWfpDKU">YouTube</a> ｜ <a href="https://www.bilibili.com/video/BV1uAZDBiEKF">哔哩哔哩</a></p>
<h2>概览</h2>
<h3>精选</h3>
<ul>
<li>Anthropic 发布 Claude Sonnet 4.6 <code>#1</code></li>
<li>xAI 上线 Grok 4.20 测试版 <code>#2</code></li>
<li>NotebookLM推出幻灯片Prompt修订与PPTX导出 <code>#3</code></li>
</ul>
<h3>模型发布</h3>
<ul>
<li>蚂蚁集团开源Ming-omni-tts音频生成模型 <code>#4</code></li>
<li>Cohere Labs发布Tiny Aya多语言模型 <code>#5</code></li>
<li>字节跳动研究团队开源 BitDance 多模态模型 <code>#6</code></li>
</ul>
<h3>开发生态</h3>
<ul>
<li>Cursor 发布 2.5 版本更新，推出插件市场 <code>#7</code></li>
<li>OpenAI修复GPT-5.3-Codex请求重定向问题 <code>#8</code></li>
<li>Cerebras下调部分免费层级的推理额度 <code>#9</code></li>
<li>Intelligent Internet 开源多Agent协作系统 Common Ground Core <code>#10</code></li>
</ul>
<h3>行业动态</h3>
<ul>
<li>Nerve加入OpenAI构建ChatGPT搜索 <code>#11</code></li>
<li>传 Moonshot AI 完成7亿美元融资 <code>#12</code></li>
</ul>
<hr />
<h2><a href="https://www.anthropic.com/news/claude-sonnet-4-6">Anthropic 发布 Claude Sonnet 4.6</a> <code>#1</code></h2>
<blockquote>
<p><strong>Anthropic</strong> 正式发布了 <code>Claude Sonnet 4.6</code> 模型。该模型在编程、长上下文推理及 <code>Agent</code> 规划能力上全面升级，并支持 <strong>100 万</strong> <code>token</code> 上下文。同步推出的还有改进版网页搜索工具，在提升准确率的同时大幅降低了 <code>Token</code> 消耗。目前，<code>Sonnet 4.6</code> 已上线 <code>API</code> 及各类AI应用，价格与上一代保持一致，免费版用户现已可在<strong>Claude</strong>体验。</p>
</blockquote>
<p><strong>Anthropic</strong> 正式发布 <code>Claude Sonnet 4.6</code>，官方称其为迄今最强的 <code>Sonnet</code> 模型。该模型在编程、长上下文推理、<code>Agent</code> 规划、知识工作及设计等领域全面升级，并提供支持 <strong>100 万</strong> token 的上下文窗口（<code>Beta版</code>）。价格维持每百万 token 输入 <strong>3</strong> 美元、输出 <strong>15</strong> 美元不变。</p>
<p>性能提升显著。在编程方面，根据 <code>Claude Code</code> 的早期测试，约 <strong>70%</strong> 的开发者更偏好 <code>Sonnet 4.6</code> 而非上代模型，<strong>59%</strong> 的用户选择它而非旗舰 <code>Opus 4.5</code>。用户反馈其在修改代码前能更有效阅读上下文，并减少“偷懒”行为。在计算机使用能力上，<code>OSWorld</code> 基准测试得分从 <strong>14.0%</strong> 大幅提升至 <strong>72.5%</strong>，能更有效地处理复杂电子表格和多步网页表单任务。据外部评估，<code>Sonnet 4.6</code> 在部分真实工作任务基准上略微优于 <code>Opus 4.6</code>。</p>
<p><strong>Anthropic</strong> 同步推出改进版 <code>Web Search</code> 和 <code>Web Fetch</code> 工具，通过 <code>代码执行</code> 对搜索结果进行动态过滤，官方数据显示平均准确率提升 <strong>11%</strong>，输入 Token 消耗减少 <strong>24%</strong>。<code>Sonnet 4.6</code> 现已上线 <code>API</code> 及各类AI应用，免费版 <strong>Claude</strong> 也可体验<code>Sonnet 4.6</code>。官方建议，对于大规模代码重构等超复杂任务，<code>Opus 4.6</code> 仍是最佳选择，但对多数任务，<code>Sonnet 4.6</code> 提供了极高性价比。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/37ae8237-3741-4ff4-950b-a7944f9f7c68/m001.png" alt="" /></p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/37ae8237-3741-4ff4-950b-a7944f9f7c68/m002.png" alt="" /></p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/37ae8237-3741-4ff4-950b-a7944f9f7c68/m003.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://www.anthropic.com/news/claude-sonnet-4-6">https://www.anthropic.com/news/claude-sonnet-4-6</a></li>
<li><a href="https://claude.com/blog/improved-web-search-with-dynamic-filtering">https://claude.com/blog/improved-web-search-with-dynamic-filtering</a></li>
</ul>
<hr />
<h2><a href="https://x.com/elonmusk/status/2023828048580387001">xAI 上线 Grok 4.20 测试版</a> <code>#2</code></h2>
<blockquote>
<p><strong>xAI</strong> 上线了 <code>Grok 4.20</code> 公开测试版，该版本引入了由四个 <code>Agent</code> 组成的 <code>原生协作系统</code>，用于处理复杂查询。据 <strong>Elon Musk</strong> 称，该版本基于 <strong>5000 亿</strong>参数的 <code>V8</code> 模型，凭借快速学习与每周迭代，<strong>下个月</strong>测试结束时，其智能水平和速度预计将比 <code>Grok 4</code> 提升约一个数量级。</p>
</blockquote>
<p><strong>xAI</strong>上线了<code>Grok 4.20</code>公开测试版，用户需在应用内手动选择。据创始人<strong>Elon Musk</strong>透露，该模型并非单纯迭代，而是基于<strong>500B</strong>参数的<code>V8</code>小型基础模型构建。官方声明指出，<code>Grok 4.2</code>基础设施支持快速学习与每周更新，以实现“<code>递归智能增长</code>”。官方预计，在下个月测试版结束时，其智能水平和速度将比<code>Grok 4</code>提升约一个数量级。</p>
<p>该版本引入的原生<code>多Agent协作系统</code>是其核心亮点。据了解，该系统包含<code>Grok/Captain</code>、<code>Harper</code>、<code>Benjamin</code>和<code>Lucas</code> <strong>四个</strong> Agent，在处理复杂查询时自动运行。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/7078bb72-ab29-4eeb-9a0a-1c0a2666c81d/m001.png" alt="" /></p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/7078bb72-ab29-4eeb-9a0a-1c0a2666c81d/m002.png" alt="" /></p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/7078bb72-ab29-4eeb-9a0a-1c0a2666c81d/m003.png" alt="" /></p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/7078bb72-ab29-4eeb-9a0a-1c0a2666c81d/m004.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://x.com/elonmusk/status/2023828048580387001">https://x.com/elonmusk/status/2023828048580387001</a></li>
</ul>
<hr />
<h2><a href="https://x.com/NotebookLM/status/2023851190102986970">NotebookLM推出幻灯片Prompt修订与PPTX导出</a> <code>#3</code></h2>
<blockquote>
<p><strong>NotebookLM</strong> 发布重要更新，现在可以直接输入提示词来微调和修改幻灯片内容。同时，系统新增了 <code>PPTX</code> 导出支持，允许用户将生成的演示文稿直接下载为 <code>PPTX</code> 文件。这两项功能目前正在向 <strong>Ultra</strong> 和 <strong>Pro</strong> 会员推送。</p>
</blockquote>
<p><code>NotebookLM</code> 发布两项重要更新：<code>Prompt-Based Revisions</code> 与 <code>PPTX Support</code>，以回应用户强烈需求。</p>
<p>核心功能 <code>Prompt-Based Revisions</code> 允许用户通过 <code>Prompt</code> 描述直接对幻灯片进行调整、定制和微调。此外，<code>NotebookLM</code> 现已支持将生成的幻灯片导出为 <code>PPTX</code> 格式，官方透露 <strong>Google Slides</strong> 的支持即将推出。<code>NotebookLM</code> 正为 <code>Ultra</code> 和 <code>Pro</code> 会员推送这两项新功能：</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/a726e5fc-b654-42ea-9d19-ca3d11dc9ace/m001.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://x.com/NotebookLM/status/2023851190102986970">https://x.com/NotebookLM/status/2023851190102986970</a></li>
</ul>
<hr />
<h2><a href="https://xqacmer.github.io/Ming-Flash-Omni-V2-TTS/">蚂蚁集团开源Ming-omni-tts音频生成模型</a> <code>#4</code></h2>
<blockquote>
<p><strong>蚂蚁集团</strong> <code>Inclusion AI</code> 开源了统一音频生成模型 <code>Ming-Omni-TTS</code>。该模型不仅能生成语音，还能合成音乐和环境音，包含 <strong>0.5B</strong> 和 <strong>16.8B-A3B</strong> 两个版本。</p>
</blockquote>
<p><strong>蚂蚁集团</strong> <strong>inclusionAI</strong> 开源统一音频生成模型 <code>Ming-omni-tts</code>，提供 <strong>0.5B</strong> 及 <code>16.8B-A3B</code> 两个版本。该模型是业界首个在单通道内联合生成语音、环境音和音乐的 <code>自回归模型</code>，通过定制 <strong>12.5Hz</strong> 连续 <code>Tokenizer</code> 实现了 <strong>3.1Hz</strong> 的高效推理帧率。官方评测显示，<code>Ming-omni-tts-16.8B-A3B</code> 在粤语生成、情感控制及零样本语音克隆等基准测试中达到 <code>SOTA</code> 水平，其文本规范化能力媲美 <code>Gemini-2.5 Pro</code>。模型权重及推理代码已上线 <strong>Hugging Face</strong>、<strong>ModelScope</strong> 及 <strong>GitHub</strong>。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/ea284cf1-f5d6-42cf-a715-7f1e71cee91c/m001.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://xqacmer.github.io/Ming-Flash-Omni-V2-TTS/">https://xqacmer.github.io/Ming-Flash-Omni-V2-TTS/</a></li>
<li><a href="https://github.com/inclusionAI/Ming-omni-tts">https://github.com/inclusionAI/Ming-omni-tts</a></li>
<li><a href="https://modelscope.cn/studios/antsipan/ming-uniaudio-demo">https://modelscope.cn/studios/antsipan/ming-uniaudio-demo</a></li>
</ul>
<hr />
<h2><a href="https://cohere.com/blog/cohere-labs-tiny-aya">Cohere Labs发布Tiny Aya多语言模型</a> <code>#5</code></h2>
<blockquote>
<p><strong>Cohere Labs</strong> 发布了名为 <code>Tiny Aya</code> 的多语言小型模型家族。该系列拥有 <strong>33.5 亿</strong> 参数，覆盖全球 <strong>70 多种</strong> 语言。</p>
</blockquote>
<p><strong>Cohere Labs</strong> 发布多语言小型模型家族 <code>Tiny Aya</code>。该系列包含 <strong>3.35B</strong> 参数基座模型及 <strong>4</strong> 个针对全球及特定区域（南亚、西亚/非洲、欧亚）优化的指令微调模型，覆盖 <strong>70+</strong> 种语言，侧重低资源语言支持。模型上下文 <strong>8K</strong>，采用 <code>CC-BY-NC</code> 协议，支持在笔记本电脑及手机端离线运行。官方指出模型擅长翻译与摘要，但在思维链推理任务上表现较弱。目前模型已在 <strong>Hugging Face</strong>、<strong>Kaggle</strong> 等平台开源，提供 <code>GGUF</code> 格式。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/88832ab7-c4cc-48f2-9407-70a7fc40e493/m001.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://cohere.com/blog/cohere-labs-tiny-aya">https://cohere.com/blog/cohere-labs-tiny-aya</a></li>
<li><a href="https://github.com/Cohere-Labs/tiny-aya-tech-report/blob/main/tiny_aya_tech_report.pdf">https://github.com/Cohere-Labs/tiny-aya-tech-report/blob/main/tiny_aya_tech_report.pdf</a></li>
<li><a href="https://huggingface.co/collections/CohereLabs/tiny-aya">https://huggingface.co/collections/CohereLabs/tiny-aya</a></li>
</ul>
<hr />
<h2><a href="https://github.com/shallowdream204/BitDance">字节跳动研究团队开源 BitDance 多模态模型</a> <code>#6</code></h2>
<blockquote>
<p><strong>字节跳动</strong>研究团队发布了名为 <code>BitDance</code> 的开源多模态模型，参数量达 <strong>140 亿</strong>，该模型专为视觉生成优化，通过 <code>并行预测 Token</code>，推理速度比标准模型提升超过 <strong>30 倍</strong>。</p>
</blockquote>
<p><strong>字节跳动</strong>研究团队近日发布开源离散自回归多模态模型 <code>BitDance</code>，参数量为 <strong>14B</strong>。模型引入<code>大词汇量二元分词器</code>及<code>下一块扩散范式</code>，支持每步并行预测最多 <strong>64</strong> 个 <code>Token</code>，官方数据显示其比标准 <code>AR 模型</code>推理速度快 <strong>30 倍</strong>以上。</p>
<p>官方发布了 <code>BitDance-14B-64x</code> 和 <code>16x</code> 两个版本，配套 <code>UniWeTok</code> 分词器。在性能方面，<code>BitDance</code> 在 <code>DPG-Bench</code>（<strong>88.28</strong> 分）和 <code>GenEval</code>（<strong>0.86</strong> 分）上表现优异。目前，该模型代码与权重已在 <strong>GitHub</strong> 和 <strong>Hugging Face</strong> 开源（<code>Apache 2.0</code>），并提供在线演示，相关论文已发布于 <strong>arXiv</strong>。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/a5632f31-cd75-4f9a-b20b-abc61940866e/m001.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://github.com/shallowdream204/BitDance">https://github.com/shallowdream204/BitDance</a></li>
<li><a href="https://bitdance.csuhan.com/">https://bitdance.csuhan.com/</a></li>
<li><a href="https://huggingface.co/collections/shallowdream204/bitdance">https://huggingface.co/collections/shallowdream204/bitdance</a></li>
</ul>
<hr />
<h2><a href="https://cursor.com/changelog/2-5">Cursor 发布 2.5 版本更新，推出插件市场</a> <code>#7</code></h2>
<blockquote>
<p><strong>Cursor</strong> 发布了 <strong>2.5</strong> 版本更新，上线了 <strong>Cursor Marketplace</strong> 插件市场。首批整合了 <strong>Figma</strong>、<strong>Stripe</strong> 和 <strong>AWS</strong> 等工具。此外，<code>子智能体</code>现在支持<code>异步运行</code>与<code>树状协作</code>，<code>沙箱功能</code>新增了<code>细粒度访问控制</code>。</p>
</blockquote>
<p>近日，代码编辑器 <strong>Cursor</strong> 正式发布 <code>2.5</code> 版本，上线了 <code>Cursor Marketplace</code> 插件市场，并对核心 <code>Agent</code> 功能与 <code>沙盒</code> 安全机制进行了升级。</p>
<p>在扩展性方面，新版本引入统一<code>插件</code>机制，将 <code>Skills</code>、<code>Subagents</code>、<code>MCP servers</code> 等能力打包。<code>Cursor Marketplace</code> 已汇集 <strong>Linear</strong>、<strong>Figma</strong>、<strong>Stripe</strong>、<strong>AWS</strong> 等首批合作伙伴插件，覆盖设计、支付、部署及数据分析全流程。用户可通过网页或编辑器内 <code>/add-plugin</code> 命令直接安装。官方已开放插件提交入口，并发布了其内部 <code>CI</code> 和 <code>代码审查</code> 工作流模板 <code>Cursor Team Kit</code>，未来将推出支持统一治理的私有团队插件市场。</p>
<p>在 <code>Agent</code> 性能方面，<code>子智能体</code> 现已支持异步运行与树状层级协作，使<code>父智能体</code>可在后台执行任务，以更低的延迟处理大型重构或多文件任务。基于此，官方推出了具备自主规划与执行能力的 <code>长期运行智能体</code>，官方称在测试中已能生成更完整的 <code>PR</code> 并减少后续干预。</p>
<p>在安全与权限控制方面，<code>沙盒</code> 新增了对域名和本地文件系统的细粒度访问控制，提供 <code>仅用户配置</code>、<code>用户配置+默认值</code> 及 <code>允许全部</code> 三种模式。企业版管理员可通过 <code>管理控制台</code> 强制实施网络策略，确保组织级的出站访问安全。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/be845020-a5ba-43a5-a15f-fac997938c77/m001.png" alt="" /></p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/be845020-a5ba-43a5-a15f-fac997938c77/m002.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://cursor.com/changelog/2-5">https://cursor.com/changelog/2-5</a></li>
<li><a href="https://cursor.com/blog/marketplace">https://cursor.com/blog/marketplace</a></li>
</ul>
<hr />
<h2><a href="https://developers.openai.com/codex/concepts/cyber-safety">OpenAI修复GPT-5.3-Codex请求重定向问题</a> <code>#8</code></h2>
<blockquote>
<p>针对部分用户使用 <code>GPT-5.3-Codex</code> 却被路由至 <code>GPT-5.2</code> 的问题，<strong>OpenAI</strong> 称已修复相关 <code>Bug</code> 并校准了 <code>分类器</code>，同时在 <code>CLI</code> <strong>v0.102.0</strong> 版本中加入了显眼的降级通知功能。</p>
</blockquote>
<p><strong>OpenAI</strong> 将 <code>GPT-5.3-Codex</code> 定义为其 <strong>Preparedness Framework</strong> 下的首个**“高网络安全能力”**模型。鉴于网络能力具备支持防御性研究与潜在恶意滥用的双重用途属性，<strong>OpenAI</strong> 实施了包括<code>安全训练</code>和<code>自动监控</code>在内的多重防护措施，会将检测到的<code>可疑网络活动流量</code> <code>重路由</code>至网络能力较弱的 <code>GPT-5.2</code> 模型。</p>
<p>针对近期用户遭遇请求被意外降级的情况，<strong>OpenAI</strong> 团队成员承认，系统曾在特定时段出现<code>过度标记问题</code>，影响了约 <strong>9%</strong> 的用户。该问题已修复，团队通过<code>校准分类器</code>将预期受影响用户比例降至 <strong>1%</strong> 以下，并修复了<code>信任访问权限</code>未生效的 <code>Bug</code>。为提升透明度，<code>CLI v0.102.0</code> 版本已加入请求被降级时的<code>显眼通知</code>，并将在未来几天内扩展至所有客户端。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/01abe632-e2e8-4802-9ff8-8b94bfbd5b27/m001.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://developers.openai.com/codex/concepts/cyber-safety">https://developers.openai.com/codex/concepts/cyber-safety</a></li>
<li><a href="https://x.com/embirico/status/2023891414623592653">https://x.com/embirico/status/2023891414623592653</a></li>
</ul>
<hr />
<h2><a href="https://inference-docs.cerebras.ai/models/overview">Cerebras下调部分免费层级的推理额度</a> <code>#9</code></h2>
<blockquote>
<p><strong>Cerebras</strong> 官方宣布，由于部分模型需求量激增，已暂时下调相关模型免费层级的 <code>速率限制</code>。</p>
</blockquote>
<p><strong>Cerebras</strong>官方宣布，因<code>zai-glm-4.7</code>和<code>qwen-3-235b-a22b-instruct-2507</code>模型需求激增，已暂时下调免费层级<code>速率限制</code>，正致力恢复原有设置。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/c0ebb726-a3e0-4afc-8ae0-a410a7df87ea/m001.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://inference-docs.cerebras.ai/models/overview">https://inference-docs.cerebras.ai/models/overview</a></li>
</ul>
<hr />
<h2><a href="https://github.com/Intelligent-Internet/CommonGround">Intelligent Internet 开源多Agent协作系统 Common Ground Core</a> <code>#10</code></h2>
<blockquote>
<p><strong>Intelligent Internet</strong> 宣布开源 <code>多 Agent</code> 协作操作系统 <code>Common Ground Core</code>，这是一个协议优先的 <code>OS</code> 内核，旨在解决 <code>多 Agent 系统</code> 常见的上下文丢失等问题。</p>
</blockquote>
<p><strong>Intelligent Internet</strong> 团队近日开源 <code>多 Agent 协作操作系统</code> <strong>Common Ground Core (CGC)</strong>。该系统定位为 <code>协议优先的 OS 内核</code>，旨在解决 <code>多 Agent</code> 扩展时的 <code>上下文丢失</code>、<code>死锁</code> 及 <code>协调崩溃</code> 等问题。<strong>CGC</strong> 采用 <code>边缘自由、内核约束</code> 设计，利用 <strong>Postgres</strong> 维护 <code>不可变共享认知账本</code> 作为 <code>真理源</code>，通过 <strong>NATS</strong> 消除 <code>分布式消息重排序风险</code>。系统将人类视为与 AI 平等的 <code>异步节点</code>，支持介入协作。目前项目已在 <strong>GitHub</strong> 发布 <code>预览版</code>，提供 <strong>Docker</strong> 部署并集成 <strong>CardBox</strong> 状态模型。官方特别提示，当前版本 <code>API</code> 无认证且具备 <code>任意命令执行能力</code>，严禁直接暴露于 <code>公网</code>。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/b928c84e-5235-405d-819a-51c14e1ad9d8/m001.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://github.com/Intelligent-Internet/CommonGround">https://github.com/Intelligent-Internet/CommonGround</a></li>
<li><a href="https://ii.inc/web/blog/post/common-ground-core-cgc">https://ii.inc/web/blog/post/common-ground-core-cgc</a></li>
</ul>
<hr />
<h2><a href="https://www.usenerve.com/blog/joining-openai">Nerve加入OpenAI构建ChatGPT搜索</a> <code>#11</code></h2>
<blockquote>
<p>初创公司 <strong>Nerve</strong> 宣布加入 <strong>OpenAI</strong>，团队将致力于在更大规模上为 <strong>ChatGPT</strong> 构建搜索功能。</p>
</blockquote>
<p>企业级 <code>AI Agent</code> 初创公司 <strong>Nerve</strong> 官方宣布加入 <strong>OpenAI</strong>，旨在为 <code>ChatGPT</code> 构建更大规模的搜索功能。<strong>Nerve</strong> 过去 <strong>两年</strong> 专注于以搜索为核心的企业级 <code>Agent</code>，因认可 <strong>OpenAI</strong> 在 <code>信息检索</code> 领域的深度与雄心而决定加入。针对现有客户，<strong>Nerve</strong> 宣布产品将在 <strong>30 天后</strong> 正式关停，即日起暂停所有计费；未来 <strong>30 天内</strong> 服务将继续运行并提供支持，过渡期结束后将安全删除所有客户数据。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/970fab9f-d63e-4427-96e6-c64530e0cae8/m001.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://www.usenerve.com/blog/joining-openai">https://www.usenerve.com/blog/joining-openai</a></li>
</ul>
<hr />
<h2><a href="https://mp.weixin.qq.com/s/MRx63AOgKZcn9ug8aSiH6w">传 Moonshot AI 完成7亿美元融资</a> <code>#12</code></h2>
<blockquote>
<p>据媒体报道，<strong>月之暗面</strong>完成<strong>7亿美元</strong>融资，<strong>阿里巴巴</strong>和<strong>腾讯</strong>参与投资，公司投后估值超过<strong>100亿美元</strong>。</p>
</blockquote>
<p>据媒体报道，<strong>Moonshot AI（月之暗面）<strong>完成</strong>7亿美元</strong>融资，投后估值超<strong>100亿美元</strong>。本轮融资由<strong>Alibaba</strong>与<strong>Tencent</strong>参与。</p>
<p>相关链接：</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/MRx63AOgKZcn9ug8aSiH6w">https://mp.weixin.qq.com/s/MRx63AOgKZcn9ug8aSiH6w</a></li>
</ul>
<hr />
<p><strong>提示</strong>：内容由AI辅助创作，可能存在<strong>幻觉</strong>和<strong>错误</strong>。</p>
<p>作者<code>橘鸦Juya</code>，视频版在同名<strong>哔哩哔哩</strong>。欢迎<strong>点赞、关注、分享</strong>。</p>
]]></content><link href="https://github.com/imjuya/juya-ai-daily/issues/1"/><published>2026-02-17T13:18:21+00:00</published></entry></feed>